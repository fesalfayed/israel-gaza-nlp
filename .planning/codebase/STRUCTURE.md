# Codebase Structure

**Analysis Date:** 2026-02-21

## Directory Layout

```
/Users/fesalfayed/Desktop/israel-gaza NLP/
├── .planning/                          # GSD planning documents
│   └── codebase/                       # Architecture reference (you are here)
├── TWAIL_Pipeline_Blueprint.md         # Complete system specification
├── test.py                             # Proof-of-concept extraction test
├── bq-results-20260221-034608-*.csv    # GDELT BigQuery export (discovery output)
└── [Planned directories below]
    ├── src/                            # Primary source code
    │   ├── discovery/                  # GDELT integration
    │   ├── acquisition/                # Article scraping
    │   ├── processing/                 # NLP extraction
    │   ├── validation/                 # Human coding framework
    │   ├── analysis/                   # Bias metrics computation
    │   └── shared/                     # Utilities and configurations
    ├── data/                           # Data storage
    │   ├── articles.db                 # SQLite database
    │   ├── queries/                    # BigQuery SQL templates
    │   └── lexicons/                   # Entity patterns and term dictionaries
    ├── notebooks/                      # Jupyter analysis notebooks
    ├── tests/                          # Unit and integration tests
    ├── config/                         # Configuration files
    ├── logs/                           # Pipeline execution logs
    └── output/                         # Final reports and visualizations
```

## Directory Purposes

**Root Level:**
- Purpose: Project root with documentation and data
- Contains: Pipeline blueprint, test scripts, BigQuery exports
- Key files: `TWAIL_Pipeline_Blueprint.md` (authoritative design specification), `test.py` (proof-of-concept)

**.planning/codebase:**
- Purpose: GSD analysis documents for this project
- Contains: ARCHITECTURE.md, STRUCTURE.md (this file), and future STACK.md, CONVENTIONS.md, TESTING.md, CONCERNS.md
- Generated by: `/gsd:map-codebase` command

**src/discovery/**
- Purpose: GDELT BigQuery integration
- Contains: SQL query templates, URL harvesting logic, deduplication utilities
- Key files: `bigquery_template.sql` (parameterized query), `harvest_urls.py` (executes query, exports CSV), `deduplicator.py` (removes syndication duplicates)
- Output: CSV file with deduplicated URL list

**src/acquisition/**
- Purpose: Full-text article extraction with fallback chain
- Contains: Newspaper3k wrapper, Selenium driver, proxy manager, LexisNexis client
- Key files: `scraper.py` (main extraction orchestrator), `newspaper_extractor.py` (Newspaper3k wrapper), `selenium_fallback.py` (Selenium + proxy rotation), `lexisnexis_client.py` (academic database export)
- Database connection: `articles` table in `data/articles.db`
- Configuration: USER_AGENTS list, request_timeout, minimum text length validation

**src/processing/**
- Purpose: Three parallel NLP extraction streams
- Contains: spaCy dependency parsing, AllenNLP SRL, custom lexicon matching
- Key files:
  - `agency_extractor.py` (subject-verb-voice extraction via spaCy)
  - `causal_extractor.py` (semantic role labeling via AllenNLP)
  - `terminology_extractor.py` (lexicon matching via spaCy PhraseMatcher)
  - `entity_classifier.py` (ISR/PAL/OTHER classification)
- Database output: `agency_annotations`, `causal_annotations`, `terminology_annotations` tables
- Model files: Downloaded from spaCy/AllenNLP registries; not committed (see .gitignore)
- Processing pattern: Iterate articles → split sentences → apply three extractors → insert results

**src/validation/**
- Purpose: Human validation framework and inter-rater agreement calculation
- Contains: Stratified sampling logic, coding interface template, agreement metrics
- Key files:
  - `sampler.py` (draws 150-sentence validation set per stratification protocol)
  - `coding_interface.py` (generates CSV template or web form for coders)
  - `agreement.py` (calculates Cohen's Kappa, precision, recall, confusion matrices)
- Database output: `validation_sample` table
- Configuration: Strata definitions (wire services, legacy print, high-casualty, diplomatic)

**src/analysis/**
- Purpose: Bias metrics computation and statistical testing
- Contains: Primary metrics (Agency Ratio, Causal Explicitness Index, Terminology Asymmetry), chi-square tests, time-series analysis, calibration adjustment
- Key files:
  - `metrics.py` (computes primary metrics)
  - `statistics.py` (chi-square, t-tests, time-series)
  - `calibration.py` (applies validation-derived error correction)
  - `visualizations.py` (matplotlib/seaborn charts)
  - `reporter.py` (generates final report.pdf)
- Output: `metrics.json`, visualizations (PNG/PDF), `report.pdf`

**src/shared/**
- Purpose: Reusable utilities and configurations
- Contains: Database connection pooling, logging setup, error handling decorators
- Key files:
  - `database.py` (SQLite context manager, connection string from env)
  - `logger.py` (configures logging to file and console)
  - `config.py` (loads environment variables, defines constants)
  - `exceptions.py` (custom exception classes)

**data/articles.db**
- Purpose: Single SQLite database for all pipeline state
- Schema: 7 tables (articles, sentences, agency_annotations, causal_annotations, terminology_annotations, validation_sample, + indexes)
- Connection string: From environment variable DATABASE_URL (default: `sqlite:///data/articles.db`)
- Constraints: Foreign keys enabled; indexes on article_id, sentence_id, publish_date

**data/queries/**
- Purpose: BigQuery SQL templates
- Key files: `gdelt_gkg_filter.sql` (parameterized discovery query)
- Parameters: date_start, date_end, sources (comma-separated), themes (comma-separated)

**data/lexicons/**
- Purpose: Entity patterns and terminology dictionaries
- Key files:
  - `isr_patterns.txt` (Israel entity mentions: israel, israeli, idf, netanyahu, etc.)
  - `pal_patterns.txt` (Palestinian entity mentions: palestinian, gaza, hamas, etc.)
  - `emotive_terms.txt` (humanizing language: massacre, slaughter, atrocity, etc.)
  - `sanitized_terms.txt` (technical language: strike, operation, precision, etc.)
- Format: One pattern per line; loaded into spaCy vocab or Python sets at startup
- Iterative refinement: Expanded from corpus observations, not static

**notebooks/**
- Purpose: Exploratory analysis and visualization
- Contents: Jupyter notebooks for corpus statistics, error analysis, temporal trends
- Examples: `corpus_overview.ipynb` (article count by source/date), `validation_agreement.ipynb` (inter-rater Kappa per coder pair)

**tests/**
- Purpose: Unit and integration test suite
- Key files:
  - `test_entity_classifier.py` (verifies ISR/PAL classification on test cases)
  - `test_agency_extractor.py` (dependency parsing on known sentences)
  - `test_database.py` (SQLite schema creation, insert/query)
  - `test_acquisition.py` (mock Newspaper3k and Selenium)
- Execution: `pytest tests/` (full suite) or `pytest tests/test_entity_classifier.py -v` (single file)

**config/**
- Purpose: Configuration management
- Key files:
  - `.env.example` (template with required environment variables)
  - `settings.py` (loads env vars, defines defaults)
- Required environment variables: DATABASE_URL, GOOGLE_APPLICATION_CREDENTIALS, LEXISNEXIS_USERNAME, LEXISNEXIS_PASSWORD, PROXY_LIST_URL

**logs/**
- Purpose: Pipeline execution logs
- Key files: `pipeline.log` (append-only; rotated daily or by size)
- Location: Configured in `src/shared/logger.py`

**output/**
- Purpose: Final reports and visualizations
- Key files:
  - `metrics.json` (primary metrics with error bars)
  - `agency_ratio_by_source.png` (faceted bar chart)
  - `terminology_asymmetry_timeline.png` (time-series line plot)
  - `report.pdf` (final write-up with tables, charts, statistical tests)

## Key File Locations

**Entry Points:**

- `src/discovery/harvest_urls.py`: Executes BigQuery query, deduplicates, exports CSV
- `src/acquisition/scraper.py`: Main script; reads CSV, calls extraction pipeline, populates articles table
- `src/processing/agency_extractor.py`, `src/processing/causal_extractor.py`, `src/processing/terminology_extractor.py`: Parallel processing; orchestrated by main script or job queue
- `src/validation/sampler.py`: Draws validation sample after processing complete
- `src/analysis/metrics.py`: Computes bias metrics; called after validation complete

**Configuration:**

- `src/shared/config.py`: Loads environment variables (DATABASE_URL, GOOGLE_APPLICATION_CREDENTIALS, etc.)
- `config/.env.example`: Template showing required variables
- `data/queries/gdelt_gkg_filter.sql`: Parameterized BigQuery discovery query

**Core Logic:**

- `src/acquisition/scraper.py`: Orchestrates extraction fallback chain (Newspaper3k → Selenium → Proxy → LexisNexis)
- `src/processing/agency_extractor.py`: spaCy dependency parsing for subject-verb-voice
- `src/processing/causal_extractor.py`: AllenNLP SRL for agent-patient-verb relationships
- `src/processing/terminology_extractor.py`: spaCy PhraseMatcher for emotive/sanitized term detection
- `src/shared/entity_classifier.py`: ISR/PAL/OTHER classification via pattern matching

**Testing:**

- `tests/test_entity_classifier.py`: Unit tests for entity classification
- `tests/test_agency_extractor.py`: Unit tests for dependency parsing
- `tests/test_database.py`: Integration tests for SQLite operations
- `tests/test_acquisition.py`: Mocked integration tests for article scraping

## Naming Conventions

**Files:**

- Python modules: `lowercase_with_underscores.py` (e.g., `agency_extractor.py`, `entity_classifier.py`)
- SQL templates: `descriptive_name.sql` (e.g., `gdelt_gkg_filter.sql`)
- Jupyter notebooks: `action_subject.ipynb` (e.g., `corpus_overview.ipynb`, `validation_agreement.ipynb`)
- Configuration: `.env` for environment variables; `settings.py` for programmatic defaults
- Lexicon files: `category_descriptors.txt` (e.g., `isr_patterns.txt`, `emotive_terms.txt`)

**Directories:**

- Functional modules: `lowercase_plural_action` (e.g., `src/discovery/`, `src/acquisition/`, `src/processing/`)
- Data artifacts: `lowercase_descriptive` (e.g., `data/queries/`, `data/lexicons/`, `logs/`, `output/`)
- Tests mirror source structure: `tests/test_module_name.py` parallels `src/module_name.py`

**Database:**

- Tables: `snake_case` (e.g., `agency_annotations`, `causal_annotations`, `validation_sample`)
- Columns: `snake_case` (e.g., `article_id`, `subject_entity`, `publish_date`)
- Indexes: `idx_table_column` (e.g., `idx_articles_source`, `idx_sentences_article`)

**Functions/Methods:**

- Extractors: `extract_<feature>` (e.g., `extract_agency()`, `extract_causal_roles()`)
- Classifiers: `classify_<category>` (e.g., `classify_entity()`)
- Utilities: `verb_<action>` (e.g., `load_model()`, `deduplicator()`)

## Where to Add New Code

**New Feature:**

- Primary code: Add module in appropriate `src/` subdirectory (e.g., `src/analysis/sentiment_analysis.py` for new metric)
- Tests: Create corresponding test file in `tests/` (e.g., `tests/test_sentiment_analysis.py`)
- Integration: Import new module in orchestration script; add to pipeline configuration
- Database: If new annotations needed, define schema in `src/shared/database.py` and add table to `articles.db`

**New Component/Module:**

- Implementation: Create directory under `src/` (e.g., `src/preprocessing/` for text cleaning)
- Key file: `src/new_component/__init__.py` with main entry point function
- Supporting files: `src/new_component/utilities.py`, `src/new_component/config.py` as needed
- Tests: `tests/test_new_component.py` with unit tests
- Documentation: Add section to `TWAIL_Pipeline_Blueprint.md` explaining design

**Utilities:**

- Shared helpers: `src/shared/utilities.py` or domain-specific file (e.g., `src/shared/nlp_utilities.py`)
- Database operations: `src/shared/database.py` (connection pooling, schema creation)
- Configuration: `src/shared/config.py` (environment variable loading, constants)
- Exceptions: `src/shared/exceptions.py` (custom exception classes)

**Lexicons:**

- New terminology categories: Add `.txt` file in `data/lexicons/` (e.g., `data/lexicons/spatial_markers.txt`)
- Entity patterns: Extend `isr_patterns.txt` or `pal_patterns.txt` as corpus observations suggest
- Loading: Modify `src/shared/entity_classifier.py` or terminology extractor to load new lexicons

## Special Directories

**data/:**
- Purpose: Artifact storage for inputs and intermediate outputs
- Generated: Yes (articles.db populated by acquisition/processing stages)
- Committed: No (articles.db and queries/ in .gitignore; lexicons/ committed for reproducibility)

**.planning/:**
- Purpose: GSD orchestrator planning documents
- Generated: Yes (written by `/gsd:map-codebase` and other orchestrator commands)
- Committed: Yes (planning docs tracked in git for project history)

**logs/:**
- Purpose: Pipeline execution logs
- Generated: Yes (pipeline.log created on first run)
- Committed: No (logs/ in .gitignore; logs are ephemeral)

**output/:**
- Purpose: Final reports and visualizations
- Generated: Yes (metrics.json, PNG/PDF charts, report.pdf produced by analysis stage)
- Committed: No (output/ in .gitignore; reports regenerated on each run)

**notebooks/:**
- Purpose: Exploratory and educational Jupyter notebooks
- Generated: Manually created by analysts
- Committed: Yes (notebooks tracked in git for transparency, cleared of output before commit)

---

*Structure analysis: 2026-02-21*
